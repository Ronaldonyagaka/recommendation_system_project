{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style= \"color:cyan\"> BUILDING A RECOMMENDATION SYSTEM </SPAN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8556\\1745788391.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mReader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_algorithms\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSVDpp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from surprise import Reader, Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.prediction_algorithms import SVD, SVDpp\n",
    "from surprise.prediction_algorithms import KNNWithMeans, KNNBasic, KNNBaseline\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy\n",
    "import math\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style= \"color:orange\"> Loading the dataset </SPAN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path, error_bad_lines = False, encoding = 'latin-1', sep=';', on_bad_lines = 'skip'):\n",
    "\n",
    "    \"A simple function that reads the data\"\n",
    "    \n",
    "    data = pd.read_csv(path, error_bad_lines = error_bad_lines, encoding = encoding, sep = sep)\n",
    "    return data\n",
    "\n",
    "book_ratings = read_data(r'C:\\Users\\user\\Documents\\Recommendation Systems\\recommendation_system_project\\BX-Book-Ratings.csv')\n",
    "books = read_data(r'C:\\Users\\user\\Documents\\Recommendation Systems\\recommendation_system_project\\BX-Books.csv')\n",
    "users = read_data(r'C:\\Users\\user\\Documents\\Recommendation Systems\\recommendation_system_project\\BX-Users.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have three datasets:\n",
    "* `books`\n",
    "* `users`\n",
    "* `rating`\n",
    "\n",
    "Let us explore them by viewing first five rows of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" calling on variable book_ratings to view the first 5 rows\"\"\"\n",
    "\n",
    "book_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" calling on variable books to view the first five rows\"\"\"\n",
    "\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" calling on variable users to view the first 5 rows\"\"\"\n",
    "\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style= \"color:orange\"> Preliminary Data understanding </SPAN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_info_shape_stats(dataset, dataset_name):\n",
    "\n",
    "    \"\"\"A simple function to check the shape, info and descriptive statistics of the dataset\"\"\"\n",
    "    \n",
    "    print('The Dataset:', dataset_name )\n",
    "    print(f\"has {dataset.shape[0]} rows and {dataset.shape[1]} columns\")\n",
    "    print('---------------------------')\n",
    "    print('---------------------------')\n",
    "    print(dataset.info())\n",
    "    print('---------------------------')\n",
    "    print('----------------------------')\n",
    "    print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"calling on the function get_info_shape_stats\"\"\"\n",
    "\n",
    "get_info_shape_stats(book_ratings, 'Book Ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"calling on the function get_info_shape_stats\"\"\"\n",
    "\n",
    "get_info_shape_stats(books, 'Books')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are columns labelled None, with numerous null values, these will be analyzed during the data cleaning stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"calling on the function get_info_shape_stats\"\"\"\n",
    "\n",
    "get_info_shape_stats(users, 'Users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_types(data, dataset_name):\n",
    "\n",
    "    \"\"\"A simple function to check the data types on th datasets \"\"\"\n",
    "\n",
    "    print(\"Dataset:\",dataset_name, \"has\",len( data.select_dtypes(include='number').columns),\n",
    "                \"Numeric columns\")\n",
    "    \n",
    "    print(\"and\", len(data.select_dtypes(include='object').columns),\n",
    "          \"Categorical columns\")\n",
    "\n",
    "    print('*****************************************************')\n",
    "    print('*****************************************************')\n",
    "\n",
    "    print('Numerical Columns:', data.select_dtypes(include='number').columns)\n",
    "    print('Categorical Coulumns:', data.select_dtypes(include='object').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" calling on the data_types function \"\"\"\n",
    "\n",
    "data_types(users, 'Users') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" calling on the data_types function \"\"\"\n",
    "\n",
    "data_types(books, 'Books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" calling on the data_types function \"\"\"\n",
    "\n",
    "data_types(book_ratings, 'Book Ratings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style= \"color:orange\"> Data Cleaning </SPAN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = []\n",
    "\n",
    "def check_duplicates(data):\n",
    "\n",
    "    \"\"\"Function that iterates through the rows of our dataset to check whether they are duplicated or not\"\"\"\n",
    "    \n",
    "    for i in data.duplicated():\n",
    "        duplicates.append(i)\n",
    "    duplicates_set = set(duplicates)\n",
    "    if(len(duplicates_set) == 1):\n",
    "        print('The Dataset has No Duplicates')\n",
    "\n",
    "    else:\n",
    "        duplicates_percentage = np.round(((sum(duplicates)/len(data)) * 100 ), 2)\n",
    "        print(f'Duplicated rows constitute of {duplicates_percentage} % of our dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(book_ratings) # checking for duplicates in book_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(books) # checking for duplicates in books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(users) # checking for duplicates in users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(data):\n",
    "\n",
    "    \"\"\" Function for checking null values in percentage in relation to length of the dataset \"\"\"\n",
    "\n",
    "    if data.isnull().any().any() == False :\n",
    "\n",
    "        print(\"There Are No Missing Values\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        missing_values = data.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "        missing_val_percent = ((data.isnull().sum()/len(data)).sort_values(ascending=False))\n",
    "\n",
    "        missing_df = pd.DataFrame({'Missing Values': missing_values, 'Percentage %': missing_val_percent})\n",
    "\n",
    "        return missing_df[missing_df['Percentage %'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values(book_ratings) # checking for missing values in book ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values(books) # checking for missing values in books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values(users) # checking for missing values in users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropping_columns(data, columns):\n",
    "\n",
    "    \"\"\"A simple function to drop columns with missing values\"\"\"\n",
    "\n",
    "    drop_column = data.drop(columns=columns, inplace = True)\n",
    "    \n",
    "    return drop_column\n",
    "\n",
    "columns_to_drop = users[['Age']]\n",
    "\n",
    "dropping_columns(users, columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(data, columns):\n",
    "    \n",
    "    \"\"\"A simple function to remove the rows of columns that have missing values \"\"\"\n",
    "    \n",
    "    new_data = data.dropna(subset=columns, inplace=True)\n",
    "    return new_data\n",
    "\n",
    "col = ['Image-URL-L', 'Publisher', 'Book-Author']\n",
    "drop_rows(books, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style= \"color:orange\"> Feature Selection and EDA </SPAN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframe(data_0, data_1, merge_column):\n",
    "    \"\"\"A function to merge the datasets based on a given column\"\"\"\n",
    "    new_df = data_0.merge(data_1, on=merge_column)\n",
    "    return new_df\n",
    "\n",
    "df_rating = merge_dataframe(users, book_ratings, \"User-ID\")\n",
    "df_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values(df_rating) # checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(df_rating) # checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_info_shape_stats(df_rating, 'Merged DataFrame') # checking the dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" merging the new dataset with the book dataset \"\"\"\n",
    "df_books = merge_dataframe(books, df_rating, 'ISBN')\n",
    "df_books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_info_shape_stats(df_books, \"Combined Dataset\") # check merged dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing_values(df_books) # check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(df_books) # check for duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity Based Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_popularity(df, column_name):\n",
    "\n",
    "    \"\"\"Calculates the popularity of values in a specific column of a dataframe\"\"\"\n",
    "\n",
    "    popularity_df = pd.DataFrame(df[column_name].value_counts())\n",
    "    return popularity_df\n",
    "\n",
    "popularity_df = calculate_popularity(df_books, 'Book-Title')\n",
    "popularity_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_active_users(dataframe, threshold):\n",
    "\n",
    "    \"\"\"Filter the dataframe to include only users who have actively rated more than the threshold\"\"\"\n",
    "    \n",
    "    # Filter the DataFrame based on the count of each unique User-ID\n",
    "    user_counts = dataframe['User-ID'].value_counts()\n",
    "    filter = user_counts > threshold\n",
    "\n",
    "    # Get the index values of the filtered rows\n",
    "    filtered_index = filter[filter].index\n",
    "\n",
    "    # Create a new DataFrame by selecting only the rows where User-ID is in the filtered index\n",
    "    filtered_df = dataframe[dataframe['User-ID'].isin(filtered_index)]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "df_filtered = filter_active_users(df_books, 300)\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rating_count(dataframe):\n",
    "\n",
    "    \"\"\"A Simple Function to Calculate the Number of Times each book has been rated\"\"\"\n",
    "\n",
    "    # Group the dataframe by 'Book-Title' and count the occurrences of 'Book-Rating' for each title\n",
    "    rating_count = dataframe.groupby('Book-Title')['Book-Rating'].count().reset_index()\n",
    "\n",
    "    # Rename the 'Book-Rating' column to 'rating_count'\n",
    "    rating_count.rename(columns={'Book-Rating': 'rating_count'}, inplace=True)\n",
    "\n",
    "    # Merge the original dataframe with the 'rating_count' dataframe based on 'Book-Title'\n",
    "    new_df = dataframe.merge(rating_count, on='Book-Title')\n",
    "\n",
    "    # Display the first few rows of the merged dataframe\n",
    "    return new_df\n",
    "\n",
    "new_book_df = calculate_rating_count(df_filtered)\n",
    "new_book_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rating_count(dataframe, threshold):\n",
    "    \n",
    "    \"\"\"A Simple Funtion to Filter the dataframe based on a minimum rating count\"\"\"\n",
    "\n",
    "    # Apply the filter to the 'dataframe' using the 'loc' function\n",
    "    filtered_df = dataframe.loc[dataframe['rating_count'] >= threshold, :]\n",
    "\n",
    "    # Display the first few rows of the filtered dataframe\n",
    "    return filtered_df\n",
    "\n",
    "rating_more_50 = filter_rating_count(new_book_df, 50)\n",
    "rating_more_50.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you preview the user ID and Book-Tittle you will get that a user has rated a book more than once. This can be brought about reading the book multiple times and having different different opinions on it. Let's preview the dataset that coontains the two columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_user_id_df = rating_more_50[['User-ID', 'Book-Title']]\n",
    "book_user_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(book_user_id_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and create the final dataframe and remove the duplicates in the two columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = rating_more_50.drop_duplicates(subset=['User-ID', 'Book-Title'])\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_info_shape_stats(final_df, 'Final DataFrame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Based Collaborative Filtering Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Collaborative filtering is a method of making automatic predictions(i.e filtering) about the interests of a user by collecting preferences or taste information from many users on the aggregate(i.e collaborating). There are two main apporoaches to collaborative filtering :\n",
    "\n",
    ">> * Item - Item CF : \"Users who like this item also liked...\"\n",
    ">> * User - Item CF : \"Users who are similar to you also liked\"\n",
    " \n",
    ">> Model based collaborative filtering approach involves building machine learning algorithms to predict user's ratings. They involve dimensionality reduction methods that reduce high dimensional matrix containing abundant number of missing values with a much smaller matrix in a lower-dimensional space.\n",
    "The goal of this section is to compare SVD and SVDpp algorithms, try optimizing parameters and explore obtained results.Let's start by preparing our dataset for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new dataframe that contains only the relevant columns \n",
    "\n",
    "final_df.rename(columns = {'User-ID':'user_id' ,'ISBN':'isbn' ,'Book-Rating':'book_rating'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Filtering out least active users \"\"\"\n",
    "\n",
    "user_ratings_threshold = 3\n",
    "\n",
    "filter_users = final_df['user_id'].value_counts()\n",
    "filter_users_list = filter_users[filter_users >= user_ratings_threshold].index.to_list()\n",
    "\n",
    "df_ratings_top = final_df[final_df['user_id'].isin(filter_users_list)]\n",
    "\n",
    "print('Filter: users with at least %d ratings\\nNumber of records: %d' % (user_ratings_threshold, len(df_ratings_top))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_ratings_threshold_perc = 0.1\n",
    "book_ratings_threshold = len(df_ratings_top['isbn'].unique()) * book_ratings_threshold_perc\n",
    "\n",
    "filter_books_list = df_ratings_top['isbn'].value_counts().head(int(book_ratings_threshold)).index.to_list()\n",
    "df_ratings_top = df_ratings_top[df_ratings_top['isbn'].isin(filter_books_list)]\n",
    "\n",
    "print('Filter: Top %d%% Most Frequently Rated Books\\nNumber of records: %d' % (book_ratings_threshold_perc*100, len(df_ratings_top)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD (Singular Value Decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> SVD is a widely used matrix decomposition method that works by reducing dimensionality of the user item matrix by extracting its latent factors and capturing underlying patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(df, model_class, rating_scale=(1, 10), cv=3):\n",
    "\n",
    "    \"\"\" A function to read our data into a Suprise Dataset format, instatiate model and perform cross validation\"\"\"\n",
    "\n",
    "    reader = Reader(rating_scale=rating_scale)\n",
    "    data = Dataset.load_from_df(df[['user_id', 'isbn', 'book_rating']], reader)\n",
    "    \n",
    "    model = model_class()\n",
    "    cv_results = cross_validate(model, data, cv=cv)\n",
    "    cv_results_df = pd.DataFrame(cv_results).mean()\n",
    "    \n",
    "    return cv_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_ratings_top.copy()\n",
    "svd_results = evaluate_model(df, SVD)\n",
    "print(\"SVD Results:\")\n",
    "print(svd_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVDpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> The SVDpp algorithm is an extension of SVD that takes into account implicit ratings.Implicit ratings refer to user interactions or behaviors that indirectly reflect their preferences or interests towards items in a recommender system.Unlike explicit ratings implicit ratings are derived from user actions such as clicks, views, purchases, time spent, or other forms of interactions with items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdpp_results = evaluate_model(df, SVDpp)\n",
    "print(\"SVDpp Results:\")\n",
    "print(svdpp_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test_RMSE for SVD is much more better. We will go ahead and do some hyperparameter tuning on the SVD model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing SVD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_ratings_top.copy()\n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "data = Dataset.load_from_df(df[['user_id', 'isbn', 'book_rating']], reader)\n",
    "\n",
    "param_grid = {\n",
    "    'n_factors': [10, 100, 500],\n",
    "    'n_epochs': [5, 20, 50], \n",
    "    'lr_all': [0.001, 0.005, 0.02],\n",
    "    'reg_all': [0.005, 0.02, 0.1]}\n",
    "\n",
    "gs_model = GridSearchCV(\n",
    "    algo_class = SVD,\n",
    "    param_grid = param_grid,\n",
    "    n_jobs = -1,\n",
    "    joblib_verbose = 5)\n",
    "\n",
    "gs_model.fit(data)\n",
    "\n",
    "# Train the SVD model with the parameters that minimise the root mean squared error\n",
    "best_SVD = gs_model.best_estimator['rmse']\n",
    "print(\"Tuned SVD Model RMSE\", gs_model.best_score['rmse'])\n",
    "print(\"Best Paramers\", gs_model.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great ! We see a reduced RMSE, this is an indication of improved performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Recommendation System "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>  Hybrid recommender system is a special type of recommender system that combines both content and collaborative filtering method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightFM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> LightFM is a hybrid matrix factorisation model representing users and items as linear combinations of their content featuresâ€™ latent factors. The model outperforms both collaborative and content-based models in cold-start or sparse interaction data scenarios (using both user and item metadata), and performs at least as well as a pure collaborative matrix factorisation model where interaction data is abundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix as cm\n",
    "import lightfm as lf\n",
    "\n",
    "# this is because I re-indexed all users and books to start from zero\n",
    "numUsers = ratings.user_id.max()+1\n",
    "numBooks = ratings.book_id.max()+1\n",
    "\n",
    "ratSparse = cm((ratings.rating, (ratings.user_id, ratings.book_id)),\n",
    "               shape=(numUsers, numBooks))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
